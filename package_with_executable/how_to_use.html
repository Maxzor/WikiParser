<b>Downloading the Wikipedia database file</b><br/><br/>

Before using Wiki Parser, the Wikipedia database file needs to be downloaded onto your computer. Wikipedia updates these database files every month or so, and they can be downloaded from Wikipedia directly <a href="https://dumps.wikimedia.org/enwiki/">here</a>. A more general description of different ways to download the database is available at <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download">here</a>.<br/><br/>

You will need the file that ends in <b>"pages-articles.xml.bz2"</b>, meaning it contains the current revisions of all articles, but no other extraneous pages. Pick the file with the latest date on it. These files are quite large; they exceeded 10 GB as of mid-2014.<br/><br/>

Another way to download them is through a BitTorrent client such as <a href="http://www.utorrent.com/">uTorrent</a>.<br/><br/>

The torrents for the English Wikipedia files are located at<br/><br/>

<a href="http://meta.wikimedia.org/wiki/Data_dump_torrents#enwiki">http://meta.wikimedia.org/wiki/Data_dump_torrents#enwiki</a><br/><br/>

<b>Important: </b>once you have downloaded the file, there is no need to decompress it. Simply leave it on disk as it was downloaded. Wiki Parser can process these bz2-compressed XML files directly.<br/><br/>

<b>Running Wiki Parser</b><br/><br/>

Once you have the Wikipedia database on your local disk, processing it with Wiki Parser is straightforward. Just start Wiki Parser, tell it where the database file is located (Step 1), indicate where to save the parsed data (Step 2), and press the Start button to begin the parse (Step 6). The full parse of English Wikipedia takes about 2-3 hours on a machine with 6-8 processor cores.<br/><br/>

After the parse completes, you can click the <b>See files in folder</b> button to navigate to the directory where the parse result files were saved. This button only appears after the parse has finished.<br/><br/>

If you'd like to modify the default parsing options, you can do so in Step 3. One important option is the <b>Test run</b> setting, which lets you quickly test the parse on a small number of articles.<br/><br/>

In addition, in Step 4 you can verify that you have enough space on your disk to run the parse, and in Step 5 you can specify how many processor cores to use.<br/><br/>

<b>The files generated by Wiki Parser</b><br/><br/>

Wiki Parser writes its output into the directory selected before the parse. The following files are written:<br/><br/>

<b>article_titles.txt</b>: the list of titles for the articles that were extracted from the Wikipedia database file and saved into XML and plain text.<br/><br/>

<b>articles_in_plain_text.txt</b>: the plain text of the parsed articles. The number of articles in this file, and the ordering of the articles is the same as in "article_titles.txt"<br/><br/>

<b>articles_in_xml.xml</b>: the parsed XML of the articles from the Wikipedia database file. This file is intended to be parsed by XML processors and isn't intended to be human-readable. The number of articles and their ordering is the same as in "article_titles.txt". The "xml_schema.txt" file contains a brief description of the XML schema used.<br/><br/>

<b>parse_report.txt</b>: a short report put together by the parser. You can look through it to verify that the parse went as expected.<br/><br/>

<b>readme.txt</b>: this description of the files generated.<br/><br/>

<b>redirects.txt</b>: in addition to parsing articles, the parser also extracts redirect information from the Wikipedia database file. The redirects are saved as "Redirect from title" --> "Redirect to title" pairs.<br/><br/>

<b>xml_schema.txt</b>: shows the XML schema for the parsed pages in "articles_in_xml.xml"<br/><br/>